{"id":"opencode-openai-proxy-757","title":"OpenAI-compatible proxy for OpenCode OAuth credentials","description":"Bun + Hono server exposing /v1/chat/completions endpoint. Uses opencode-auth-provider for OAuth credentials. Full streaming and tool calling support.","status":"closed","priority":1,"issue_type":"epic","created_at":"2026-01-16T14:55:02.168755078+03:00","created_by":"dvystavnoy","updated_at":"2026-01-24T17:00:07.0099755+03:00","closed_at":"2026-01-24T17:00:07.0099755+03:00","close_reason":"Closed"}
{"id":"opencode-openai-proxy-757.1","title":"Foundation: types, errors, runtime, server skeleton","description":"## Context\nBase infrastructure for the OpenAI-compatible proxy.\n\n## Key Changes\n- `src/types.ts` — OpenAI request/response/chunk types\n- `src/errors.ts` — Error mapping with Vercel AI SDK `.isInstance()` detection + fallback\n- `src/runtime.ts` — OpencodeAI singleton, parseModel helper\n- `src/index.ts` — Hono server skeleton with error middleware, /health endpoint\n\n## Patterns to Follow\n- Error mapping: APICallError.statusCode for HTTP status, fallback to api_error 500\n- See research.md for 30 Vercel AI SDK error classes\n\n## Success Criteria\n- [ ] `bun run src/index.ts` starts without errors\n- [ ] `curl http://localhost:8080/health` returns `{\"status\":\"ok\"}`\n- [ ] `mapToOpenAIError(new Error(\"test\"))` returns api_error 500\n\n## References\n- Full plan: `.beads/artifacts/opencode-openai-proxy-757/plan.md`\n- Research: `.beads/artifacts/opencode-openai-proxy-757/research.md`","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-16T20:01:44.971474363+03:00","created_by":"dvystavnoy","updated_at":"2026-01-16T20:11:11.163446276+03:00","closed_at":"2026-01-16T20:11:11.163456892+03:00","dependencies":[{"issue_id":"opencode-openai-proxy-757.1","depends_on_id":"opencode-openai-proxy-757","type":"parent-child","created_at":"2026-01-16T20:01:44.973048461+03:00","created_by":"dvystavnoy"}]}
{"id":"opencode-openai-proxy-757.2","title":"Non-streaming completions: request/response converters","description":"## Context\nWorking /v1/chat/completions for non-streaming requests.\n\n## Key Changes\n- `src/converters/request.ts` — OpenAI messages/tools → Vercel AI SDK format\n- `src/converters/response.ts` — GenerateTextResult → OpenAI ChatCompletion\n- `src/routes/completions.ts` — Handler using generateText()\n- Update `src/index.ts` — Mount completions route\n\n## Patterns to Follow\n- Use `jsonSchema()` helper for tool parameters (avoids Zod conversion)\n- Stringify toolCalls.args to JSON for OpenAI format\n- finishReason: \"tool-calls\" → \"tool_calls\" (hyphen to underscore)\n\n## Success Criteria\n- [ ] Basic completion works via curl\n- [ ] Unit tests for converters pass\n\n## References\n- Full plan: `.beads/artifacts/opencode-openai-proxy-757/plan.md`","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-16T20:01:52.509283765+03:00","created_by":"dvystavnoy","updated_at":"2026-01-23T16:59:16.6745931+03:00","closed_at":"2026-01-23T16:59:16.6745931+03:00","dependencies":[{"issue_id":"opencode-openai-proxy-757.2","depends_on_id":"opencode-openai-proxy-757","type":"parent-child","created_at":"2026-01-16T20:01:52.537694435+03:00","created_by":"dvystavnoy"}]}
{"id":"opencode-openai-proxy-757.3","title":"Streaming: SSE stream converter","description":"## Context\nSSE streaming support for /v1/chat/completions.\n\n## Key Changes\n- `src/converters/stream.ts` — fullStream → OpenAI SSE chunks\n- Update `src/routes/completions.ts` — streamText() + streamSSE()\n\n## Critical Details\n- Mid-stream errors: Send as SSE event `data: {\"error\": {...}}`, NOT HTTP status\n- OpenAI clients expect error events, will raise exception\n- After error event, close stream (no [DONE])\n- Use Hono's `streamSSE()` helper\n\n## Patterns to Follow\n- First chunk: `delta.role: \"assistant\"`\n- text-delta → `delta.content`\n- finish → `finish_reason`, then `data: [DONE]`\n\n## Success Criteria\n- [ ] Streaming works via curl with `\"stream\": true`\n- [ ] Stream ends with `data: [DONE]`\n\n## References\n- Full plan: `.beads/artifacts/opencode-openai-proxy-757/plan.md`","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-16T20:01:54.235902245+03:00","created_by":"dvystavnoy","updated_at":"2026-01-23T17:02:20.5305671+03:00","closed_at":"2026-01-23T17:02:20.5305671+03:00","dependencies":[{"issue_id":"opencode-openai-proxy-757.3","depends_on_id":"opencode-openai-proxy-757","type":"parent-child","created_at":"2026-01-16T20:01:54.241669865+03:00","created_by":"dvystavnoy"}]}
{"id":"opencode-openai-proxy-757.4","title":"Tool calling: format conversion, multi-turn support","description":"## Context\nFull tool calling support with multi-turn conversations.\n\n## Key Changes\n- Update `src/converters/request.ts` — tool message conversion with toolName tracking\n- Update `src/converters/response.ts` — toolCalls array conversion\n- Update `src/converters/stream.ts` — tool-call parts in stream\n\n## Critical Details\n- OpenAI tool messages lack `toolName`, Vercel requires it\n- Solution: Build `Map\u003ctool_call_id, toolName\u003e` from assistant messages\n- convertMessages() must process ENTIRE array, not individual messages\n- Error if tool_call_id not found in map\n\n## Patterns to Follow\n- OpenAI: `{ role: \"tool\", tool_call_id, content }`\n- Vercel: `{ role: \"tool\", content: [{ type: \"tool-result\", toolCallId, toolName, result }] }`\n\n## Success Criteria\n- [ ] Tool calling works via curl with tools array\n- [ ] Multi-turn tool conversation works\n\n## References\n- Full plan: `.beads/artifacts/opencode-openai-proxy-757/plan.md`","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-16T20:01:59.364387567+03:00","created_by":"dvystavnoy","updated_at":"2026-01-24T15:53:13.332909+03:00","closed_at":"2026-01-24T15:53:13.332909+03:00","dependencies":[{"issue_id":"opencode-openai-proxy-757.4","depends_on_id":"opencode-openai-proxy-757","type":"parent-child","created_at":"2026-01-16T20:01:59.371373092+03:00","created_by":"dvystavnoy"}]}
{"id":"opencode-openai-proxy-757.5","title":"Models endpoint + E2E test with openai client","description":"## Context\n/v1/models endpoint and E2E validation with real OpenAI client.\n\n## Key Changes\n- `src/routes/models.ts` — GET /v1/models handler\n- `tests/e2e-openai-client.ts` — E2E with official `openai` npm package\n- Add `openai` to devDependencies\n\n## E2E Test Requirements\n- Use official `openai` npm package (NOT our own HTTP calls)\n- Configure `baseURL: \"http://localhost:8080/v1\"`\n- Test 1: Basic completion (non-streaming)\n- Test 2: Streaming completion\n- Test 3: Tool calling with multi-turn conversation\n\n## Why External Client\n- Validates wire format compatibility\n- If `openai` package works, LiteLLM/Langchain/aider will work\n\n## Success Criteria\n- [ ] `/v1/models` returns model list\n- [ ] `bun run test:e2e` passes with openai client\n\n## References\n- Full plan: `.beads/artifacts/opencode-openai-proxy-757/plan.md`","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-16T20:02:02.093881339+03:00","created_by":"dvystavnoy","updated_at":"2026-01-24T15:56:01.4096354+03:00","closed_at":"2026-01-24T15:56:01.4096354+03:00","dependencies":[{"issue_id":"opencode-openai-proxy-757.5","depends_on_id":"opencode-openai-proxy-757","type":"parent-child","created_at":"2026-01-16T20:02:02.102205455+03:00","created_by":"dvystavnoy"}]}
